### 爬虫的概念

### 爬虫的流程
 - url --- 发送请求 --- 

### http请求形式
 - 请求方法　url　
 
### http常见请求头
１．host 主机和端口号
２．Connection 链接类型
３．Upgrade-Insecure-Requests: 1　　升级为ＨＴＴＰＳＱＩＮＧＱＩＵ
4. User-Agent  浏览器名称
5. Accept  浏览器名称 
6. Referer 页面跳转
7. Accept-Encoding 文件编码格式
8. Cookie 
9. x-requested-with XMLHttpRestquest 是ajax异步请求

### 爬虫要根据当前的url地址对应的响应为准，当前的url地址的elements内容和url的响应不一样

### 判断请求时候成功

### requests编码


### 发送带参数的请求
 - 参数形式: 字典
 - kw={'wd':'长城'}
 - 用法：　requests.get(url, params=kw)
 
### 字符串格式化的另一种方式
```python

"创{}智{}播客".format(11,22,22)
'创11智22播客'

```

### 爬虫发送post请求
```
res = requests.post(url)

data = data.headers = headers

data的格式 字典
res = requests.post(post_url, data=data, headers=headers)

```

### 使用代理
```
用法　requests.get(url, proxies=proxies)

proxies的形式: 字典

proxies = {
    "http":"http://12.34.56.79:9588",
    "https":"https://12.34.56.79:9588"
}
-准备一堆可用的ip进行随机使用
-如何随机选择代理ip，让使用次数较少的ip地址有更大的可能性被用到
 - {"ip": ip, "times":0}
 - [{},{},{},{},{},{}],对这个ip的列表进行排序，　按照使用次数进行排序
 - 
-检查ip可用性
 - 可以使用requests添加超时操作，判断ip地址质量
 - 在线代理ip质量检测网站

```

###　携带cookie请求
 - 携带一堆cookie请求，把cookie组成cookie池

 
### 请求登陆之后的网站
 - 实例化session
 - 先使用session发送请求，登录网站，把cookie保存session中
 - 在使用session请求登录之后才能访问的网站, session能够自动的携带登陆成功是保存在哪其中的cookie，进行请求 
 - session = requests.session()
 - response = session.get(url, headers)
 
### 不发送post请求，使用ｃｏｏｋｉｅ获取登录之后的页面
- cookie过期时间很长的网站
- cookie过期之后能够拿到的你所有数据，比较麻烦
- 配合其他程序一起使用，其他程序专门获取ｃｏｏｋｉｅ，房钱程序专门请求页面

### 字典推导式
```python
cookies = "anonymid=k9s0c65y2om8pj; _r01_=1; taihe_bi_sdk_uid=dc98311ffc7de226cb1f79afdd705b6f; jebe_key=bf8a72df-0519-4da7-b1df-362082dfd8d0%7C18333670b0ee1f54e006adc80fdd7f23%7C1588568014097%7C1%7C1588568014407; jebe_key=bf8a72df-0519-4da7-b1df-362082dfd8d0%7C92e70cb2bde7172939cddc8f15fdd3b4%7C1588663804895%7C1%7C1588663805116; depovince=GW; JSESSIONID=abcQCcacpRCLo6Lv4O4qx; ick_login=c4088f0e-05b4-45aa-bdf0-9b08f1473ad5; taihe_bi_sdk_session=02916b8090c4c5873f420e06488a9481; ick=2cb82a29-88ff-4992-af3c-666c7117e519; __utma=151146938.1079666667.1598678110.1598678110.1598678110.1; __utmc=151146938; __utmz=151146938.1598678110.1.1.utmcsr=renren.com|utmccn=(referral)|utmcmd=referral|utmcct=/; _de=D5FD513C20B9124F1FF9E00605E6865D; __utmt=1; __utmb=151146938.5.10.1598678110; jebecookies=a83fe94f-bb8d-4a00-90ea-c063cdb4a65a|||||; p=32d0ca098d880623d09023785ef7117d8; first_login_flag=1; ln_uact=15701229789; ln_hurl=http://head.xiaonei.com/photos/0/0/men_main.gif; t=028857504aa85b7281b1625a0bee47098; societyguester=028857504aa85b7281b1625a0bee47098; id=974361808; xnsid=c74ef2cb; ver=7.0; loginfrom=null; wp_fold=0"
cookies = {i.split("=")[0]: i.split("=")[1] for i in cookies.split("; ")}
```
```python
[self.url_temp.format(i*50) for i in range(1000)]


```
### 获取登录后的页面的三种方式
- 实例化session，使用session发送post请求，在使用他获取登录后的页面
- headers中添加cookie键，值为字符串
- 在请求方法中添加cookies参数，接受字典形式的cookie．字典形式的cookie中的键是cookie的name, 值是cookie的 value

[python-requests文档](https://requests.readthedocs.io/en/master/)
[python-requests中文文档](https://2.python-requests.org/zh_CN/latest/)
 
 
### 寻找登录的post地址
- 在from表单中寻找action对应的url地址
- post的数据是input标签中的name得知作为键，真正的用户名密码作为键的字典，post的url地址就是action中对应的url地址

-抓包，寻找登录的url地址
    - 勾选perserve　log地址，防止页面跳转找不到url 
    - 寻找post数据，确定参数
        -　参数不会变，直接用，比如密码不是动态加密的 
        -　参数会变
            - 参数在响应中 
            -  通过js生成
   
### 定位想要的js   
- 选择会触发js时间的按钮，点击event listener,找到js的位置 
- 通过chrome中的search all file来搜索url中的关键字
- 通过断电的方式查看js的操作

 
## requests小技巧
- 1. requests.util.dict_from_cookiejar把cookie对象转化为字典 
-  2. 请求SSL证书验证
    response = requests.get("https://www.12306.cn/mormhweb", verify=Flase)
-  设置超时
    response = requests.get(url, timeout=10)
-  4. 配合状态码判断是否 请求成功
    assert response.status_code == 200
-  

### 数据提取之json
```python
import requests
import json
from pprint import pprint

headers = {
    "User-Agent":"Mozilla/5.0 (Linux; Android 5.0; SM-G900P Build/LRX21T) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Mobile Safari/537.36",
    "Cookie":"ll='108288'; bid=51rz71pD9Bg; ap_v=0,6.0; __utma=30149280.425351927.1599285204.1599285204.1599285204.1; __utmc=30149280; __utmz=30149280.1599285204.1.1.utmcsr=baidu|utmccn=(organic)|utmcmd=organic; _vwo_uuid_v2=D0F6F3969D5FE3CCF35168329EAD8DE9C|34915b99f4d3cc72d500b6d61ff95dd8; Hm_lvt_6d4a8cfea88fa457c3127e14fb5fabc2=1599285498; _ga=GA1.2.425351927.1599285204; _gid=GA1.2.1833855976.1599285500; UM_distinctid=1745cddd5d2485-02c04a6016b6bd-631f1934-38400-1745cddd5d354c; Hm_lvt_19fc7b106453f97b6a84d64302f21a04=1599285877; Hm_lpvt_19fc7b106453f97b6a84d64302f21a04=1599285877; talionnav_show_app='0'; Hm_lpvt_6d4a8cfea88fa457c3127e14fb5fabc2=1599286001; _ga=GA1.3.425351927.1599285204; _gid=GA1.3.1833855976.1599285500",
    "Referer":"https://m.douban.com/movie/nowintheater?ioc_id=108288"
}


url = "https://m.douban.com/rexxar/api/v2/subject_collection/movie_showing/items?os=android&for_mobile=1&start=0&count=18&loc_id=108288"


res = requests.get(url, headers=headers)

# json.loads把json字符串转化为python类型
ret = json.loads(res.content.decode())
pprint(ret)


# json.dumps能够把python类型转化为json字符串
# 写入之前需要说明文件的编码
# 写入之后需要使用ensure_ascii=False让编码显示中文
# indent自动显示每个字段的空格
with open('historyReptile/douban.json', 'w', encoding='utf-8') as f:
    f.write(json.dumps(ret, ensure_ascii=False, indent=4))
```

### json使用注意点
```
- json中的字符串都是双引号引起来的
- 如果不是双引号
    - eval: 能实现简单的字符串和python类型的转化
    - replace:　把单引号替换为双引号
```
### 正则表达式复习
```python
re.complie 编译
pattern.match 从头找一个
pattern.search 找一个
pattern.fildall 找所有
pattern.sub 替换

```
### XPATH LXML类库
```python
nodename    选取此节点的所有子节点 
/   从根节点选取
//  从匹配选择的当前节点选择文档中的节点，而不考虑他们的位置
.   选取当前节点
..  选取当前的父节点
@   选取属性

```
### xpath学习重点
```
- 使用xpath helper或者是chrome中的copy xpath 
- 获取文本
    - `a/text()` a标签下的文本
    - `a//text()`　a标签下的所标签的文本
    - //a[text()='下一页']　选择啊标签中文本是下一页的a标签

 - ｀＠符号｀
    - a/@href
    - //ul[@id="detail-list"]

- `//`
    -　在xpath开始的时候表示从当前的html中任意位置开始选择
    -　`li//a`表示的事li下任何一个标签

### xpath的节点选择语法
/bookstore/book[1]  选取属于bookstrore的第一个book元素

/bookstore/book[last()]选取属于bookstrore的最后一个book元素

/bookstore/book[last() - 1]

/bookstore/book[position() < 3]

//title[@lang]　选取所有拥有名lang的属性的title元素

//title[@lang='eng']

/bookstore/book[price > 35.00]

/bookstore/book[price > 35.00]/title    选取bookstrore元素中的book元素的所有title元素，且其中的price元素的值须大于３５．００



*       匹配任何元素节点
@*      匹配任何属性节点
node()  匹配任何类型的节点


/bookstorke/*       选取bookstrore元素的所有子元素
//*                 选取文档中的所有元素
html/node()/meta/@* 选择html下面任意节点的meta节点的所有属性
//title[@*]         选取所有带有属性的title元素


```
### lxml库
```
安装lxml

pip3 install wheel (必须安装) 
pip3 install lxml 
使用入门
- 导入lxml的etree库
    from lxml import etree
- 利用etree.HTML,将字符串转化成element对象
- Elemnet对象具有xpath的方法
- html = etree.HTML(text)
- html.xpath("") 在括号中写入xpath语句可以使用
lxml 可以自动修正html代码




# 提取页面数据的思路
　- 先分组，渠道
　- 遍历，取每一组进行数据的提取，
　- 先分组
　- 先分组

```
```python
# -*- coding:utf-8 -*-
from lxml import etree

text = '''
    <section id="list" class="grid">
<a href="https://m.douban.com/movie/subject/27126336/" class="item">
    <div class="cover">
        <div class="wp ratio3_4">
            <img src="https://img2.doubanio.com/view/photo/m_ratio_poster/public/p2616924633.jpg" alt="假面饭店" data-x="1080" data-y="1463" class="img-show" style="width: 100%;">
            
        </div>
    </div>

    <div class="info">
        <h4></h4>
        <h3> </h3>
        <p class="rank">
                    <span class="rating-stars" data-rating="3.2"><span class="rating-star rating-star-small-full"></span><span class="rating-star rating-star-small-full"></span><span class="rating-star rating-star-small-full"></span><span class="rating-star rating-star-small-gray"></span><span class="rating-star rating-star-small-gray"></span></span> <span>6.4</span>
        </p>
        <p class="meta">日本 / 剧情 犯罪 悬疑 / 铃木雅之 / 木村拓哉 长泽雅美</p>
        <cite></cite>
    </div>
</a>
<a href="https://m.douban.com/movie/subject/26348103/" class="item">
    <div class="cover">
        <div class="wp ratio3_4">
            <img src="https://img9.doubanio.com/view/photo/m_ratio_poster/public/p2616349755.jpg" alt="小妇人" data-x="1080" data-y="1512" class="img-show" style="width: 100%;">
            
        </div>
    </div>
    <div class="info">
        <h4></h4>
        <h3>小妇人 </h3>
        <p class="rank">
                    <span class="rating-stars" data-rating="4.0"><span class="rating-star rating-star-small-full"></span><span class="rating-star rating-star-small-full"></span><span class="rating-star rating-star-small-full"></span><span class="rating-star rating-star-small-full"></span><span class="rating-star rating-star-small-gray"></span></span> <span>8.1</span>
        </p>
        <p class="meta">美国 / 剧情 爱情 / 格蕾塔·葛韦格 / 西尔莎·罗南 艾玛·沃森</p>
        <cite></cite>
    </div>
</a>
<a href="https://m.douban.com/movie/subject/30252495/" class="item">
    <div class="cover">
        <div class="wp ratio3_4">
            <img src="https://img9.doubanio.com/view/photo/m_ratio_poster/public/p2615015805.jpg" alt="1917" data-x="3000" data-y="4200" class="img-show" style="width: 100%;">
            
        </div>
    </div>

    <div class="info">
        <h4></h4>
        <h3>1917 </h3>
        <p class="rank">
                    <span class="rating-stars" data-rating="4.3"><span class="rating-star rating-star-small-full"></span><span class="rating-star rating-star-small-full"></span><span class="rating-star rating-star-small-full"></span><span class="rating-star rating-star-small-full"></span><span class="rating-star rating-star-small-gray"></span></span> <span>8.5</span>
        </p>
        <p class="meta">美国 英国 印度 西班牙 加拿大 中国大陆 / 剧情 战争 / 萨姆·门德斯 / 乔治·麦凯 迪恩·查尔斯·查普曼</p>
        <cite></cite>
    </div>
</a>
<a href="https://m.douban.com/movie/subject/30170833/" class="item">
    <div class="cover">
        <div class="wp ratio3_4">
            <img src="https://img1.doubanio.com/view/photo/m_ratio_poster/public/p2616740389.jpg" alt="荞麦疯长" data-x="5906" data-y="8268" class="img-show" style="width: 100%;">
            
        </div>
    </div>

    <div class="info">
        <h4></h4>
        <h3>荞麦疯长 </h3>
        <p class="rank">
                    <span class="rating-stars" data-rating="2.3"><span class="rating-star rating-star-small-full"></span><span class="rating-star rating-star-small-full"></span><span class="rating-star rating-star-small-gray"></span><span class="rating-star rating-star-small-gray"></span><span class="rating-star rating-star-small-gray"></span></span> <span>4.5</span>
        </p>
        <p class="meta">中国大陆 / 剧情 爱情 / 徐展雄 / 马思纯 钟楚曦</p>
        <cite></cite>
    </div>
</a>
<a href="https://m.douban.com/movie/subject/26661193/" class="item">
    <div class="cover">
        <div class="wp ratio3_4">
            <img src="https://img3.doubanio.com/view/photo/m_ratio_poster/public/p2614225081.jpg" alt="我在时间尽头等你" data-x="1080" data-y="1512" class="img-show" style="width: 100%;">
            
        </div>
    </div>

    <div class="info">
        <h4></h4>
        <h3>我在时间尽头等你 </h3>
        <p class="rank">
                    <span class="rating-stars" data-rating="2.5"><span class="rating-star rating-star-small-full"></span><span class="rating-star rating-star-small-full"></span><span class="rating-star rating-star-small-gray"></span><span class="rating-star rating-star-small-gray"></span><span class="rating-star rating-star-small-gray"></span></span> <span>5.1</span>
        </p>
        <p class="meta">中国大陆 / 爱情 奇幻 / 姚婷婷 / 李鸿其 李一桐</p>
        <cite></cite>
    </div>
</a>
</section>
'''

html = etree.HTML(text)

# print(html.tag)
# 查看element对象中包含的字符串
# print(etree.tostring(html, encoding="UTF-8").decode())
# rettitle = html.xpath("//a[@class='item']//div[@class='info']/h3/text()")
#
# print(rettitle)
# ret = html.xpath("//a[@class='item']//p[@class='meta']/text()")
# print(ret)
#
# # 每一条新闻，把url和文本组成字典
# for txt in ret:
#     item = {}
#     item['star'] = txt
#     item['title'] = rettitle.index()
#     print(item)

ret3 = html.xpath("//a[@class='item']")
print(ret3)

for i in ret3:
    item = {}
    item["title"] = i.xpath(".//div[@class='info']/h3/text()")[0] if len(i.xpath("//div[@class='info']/h3/text()")) > 0 else None
    item["star"] = i.xpath(".//p[@class='meta']/text()")[0] if len("//p[@class='meta']/text()") > 0 else None
    print(item)
```

### xpath中的包含的用法
- `//div[contains(@class, 'i')]`

### 实现爬虫的套路
 - 准备url
     - 准备start_url
     - ｕｒｌ地址规律不明显，总数不确定
     - 通过代码提取下一页的url
     - ｘｐａｔｈ
     - 寻找url
     
     - 准备url_list
         - 页码总数明确
         - 地址规律明显
         
 - 发送请求，获取响应
     - 添加随机的user-agent,反反爬虫
     - 添加随机的的代理ｉｐ，发发爬虫
     - 在对方判断出我们还是爬虫之后，应该添加更多的headers字段，包括ｃｏｏｋｉｅ
     
     - ｃｏｏｋｉｅ的处理可以使用ｓｅｓｓｉｏｎ来解决
     - 准备一堆能用的ｃｏｏｋｉｅ组成ｃｏｏｋｉｅ池
        -　不登录 
            -准备刚开始能够请求陈宫的对方的ｃｏｏｋｉｅ，即接受对方网站设置在ｒｅｓｐｏｎｓｅ的ｃｏｏｋｉｅ 
            -下一次请求, 使用之前的列表中的ｃｏｏｋｉｅ来请求
        -如果登录 
            -　准备多个账号 
            - 使用程序获取每个账号的ｃｏｏｋｉｅ
            - 
     - 
-　提取数据 
    -　确定数据的位置 
        -　如果数据在当前url中
            - 如果是列表页的数据
            - 如果是详情页的数据
                - １．　确定url
                - 2.　发送请求
                - ３．提取数据
                - ４．返回数据
                - 
        -　如果当前数据不在当前ｕｒｌ中 
            -　在其他的相应中寻找数据的位置 
                -　１．　从ｎｅｔｗｏｒｋ中从上往下找 
            -　 
   -数据的提取
        - xpath,从ｈｔｍｌ中整块的数据，先分组，之后每一组在提取
        - ｒｅ　提取ｍａｘ＿ｔｉｍｅ，ｐｒｉｃｅ，ｈｔｍｌ中的json字符串
        - 使用ｃｈｒｏｍｅ的ｓｅａｒｃｈ　ａｌｌ　ｆｉｌｅ　搜做数字和英文

- 保存
    -　保存到本地，text,json,csv 
    -　保存到数据库
    -　 
- 
- 
[selenium使用](https://selenium-python-zh.readthedocs.io/en/latest/waits.html)

### 安装使用selenium
[所有版本chromedriver下载](http://chromedriver.storage.googleapis.com/index.html)
[selenium之 chromedriver与chrome版本映射表](https://blog.csdn.net/huilan_same/article/details/51896672)
> 下载chrome dirver，然后放置到/usr/local/bin的目录下，再次运行就OK了
[]()
[]()
>
### 验证码的识别
```
- url不变,验证码不变,
    - 请求的验证码的地址,获得响应,识别
- url不变,验证码会变
    - 
- 1. 实例化session
- 2. 使用session请求登录页面,获取验证码的地址
- 3. 使用session请求验证码, 识别
- 4. send request from session


- 使用selenium登录,遇到验证码
    - url不变验证码不变 同上
    -  url不变,验证码会变
        -   1. selenium请求登录页面,同时拿到验证码的地址
        -   1. 获取登录页面ong de driver中的cookie,交给requests模块发送验证码的请求,识别
        -   1. 输入验证码,点击登录
    -  
- 
```
### selenium使用的注意点
```
- 获取文本和获取属性
    - 先定位元素然后获取 .text 获取 get_attribute方法 
- selenium和find_elements的区别 
- find_element  和find_elements的区别
    -  find_element返回的是一个element,如果没有就会报错
    -  find_elements返回一个列表,没有就是空列表
    - 在判断是否有下一页的时候,使用find_elements来根据结果的列表页面长度来判断 
- 如果页面中含有iframe frame需要先调
    - 用driver.switch_to.frame的方法切换到frame定位元素

- selenium第一页的数据会等待加载完成再去获取数据,翻页之后就不会等待数据加载完成,所以需要timesleep(3)
- find_element_bu_class只能传一个class

```
```python
from selenium import webdriver
import time

driver = webdriver.Chrome()

driver.get("https://mail.qq.com/")

# 切换到iframe
driver.switch_to.frame("login_frame")

driver.find_element_by_id("u").send_keys("840254112@qq.com")

time.sleep(5)
driver.quit()
```

### 安装tresseract 识别图片中的文字
```
tressact是一个将图像翻译成文字的ocr库(光学问题识别Optical Character Recognition)

安装:
sudo apt-get install tesseract-ocr

在python中调用 Tesseract
pip install pytesseract

```
 
 
 
 
 
 
