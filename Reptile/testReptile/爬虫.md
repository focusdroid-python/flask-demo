### 爬虫的概念

### 爬虫的流程
 - url --- 发送请求 --- 

### http请求形式
 - 请求方法　url　
 
### http常见请求头
１．host 主机和端口号
２．Connection 链接类型
３．Upgrade-Insecure-Requests: 1　　升级为ＨＴＴＰＳＱＩＮＧＱＩＵ
4. User-Agent  浏览器名称
5. Accept  浏览器名称 
6. Referer 页面跳转
7. Accept-Encoding 文件编码格式
8. Cookie 
9. x-requested-with XMLHttpRestquest 是ajax异步请求

### 爬虫要根据当前的url地址对应的响应为准，当前的url地址的elements内容和url的响应不一样

### 判断请求时候成功

### requests编码


### 发送带参数的请求
 - 参数形式: 字典
 - kw={'wd':'长城'}
 - 用法：　requests.get(url, params=kw)
 
### 字符串格式化的另一种方式
```python

"创{}智{}播客".format(11,22,22)
'创11智22播客'

```

### 爬虫发送post请求
```
res = requests.post(url)

data = data.headers = headers

data的格式 字典
res = requests.post(post_url, data=data, headers=headers)

```

### 使用代理
```
用法　requests.get(url, proxies=proxies)

proxies的形式: 字典

proxies = {
    "http":"http://12.34.56.79:9588",
    "https":"https://12.34.56.79:9588"
}
-准备一堆可用的ip进行随机使用
-如何随机选择代理ip，让使用次数较少的ip地址有更大的可能性被用到
 - {"ip": ip, "times":0}
 - [{},{},{},{},{},{}],对这个ip的列表进行排序，　按照使用次数进行排序
 - 
-检查ip可用性
 - 可以使用requests添加超时操作，判断ip地址质量
 - 在线代理ip质量检测网站

```

###　携带cookie请求
 - 携带一堆cookie请求，把cookie组成cookie池

 
### 请求登陆之后的网站
 - 实例化session
 - 先使用session发送请求，登录网站，把cookie保存session中
 - 在使用session请求登录之后才能访问的网站, session能够自动的携带登陆成功是保存在哪其中的cookie，进行请求 
 - session = requests.session()
 - response = session.get(url, headers)
 
### 不发送post请求，使用ｃｏｏｋｉｅ获取登录之后的页面
- cookie过期时间很长的网站
- cookie过期之后能够拿到的你所有数据，比较麻烦
- 配合其他程序一起使用，其他程序专门获取ｃｏｏｋｉｅ，房钱程序专门请求页面

### 字典推导式
```python
cookies = "anonymid=k9s0c65y2om8pj; _r01_=1; taihe_bi_sdk_uid=dc98311ffc7de226cb1f79afdd705b6f; jebe_key=bf8a72df-0519-4da7-b1df-362082dfd8d0%7C18333670b0ee1f54e006adc80fdd7f23%7C1588568014097%7C1%7C1588568014407; jebe_key=bf8a72df-0519-4da7-b1df-362082dfd8d0%7C92e70cb2bde7172939cddc8f15fdd3b4%7C1588663804895%7C1%7C1588663805116; depovince=GW; JSESSIONID=abcQCcacpRCLo6Lv4O4qx; ick_login=c4088f0e-05b4-45aa-bdf0-9b08f1473ad5; taihe_bi_sdk_session=02916b8090c4c5873f420e06488a9481; ick=2cb82a29-88ff-4992-af3c-666c7117e519; __utma=151146938.1079666667.1598678110.1598678110.1598678110.1; __utmc=151146938; __utmz=151146938.1598678110.1.1.utmcsr=renren.com|utmccn=(referral)|utmcmd=referral|utmcct=/; _de=D5FD513C20B9124F1FF9E00605E6865D; __utmt=1; __utmb=151146938.5.10.1598678110; jebecookies=a83fe94f-bb8d-4a00-90ea-c063cdb4a65a|||||; p=32d0ca098d880623d09023785ef7117d8; first_login_flag=1; ln_uact=15701229789; ln_hurl=http://head.xiaonei.com/photos/0/0/men_main.gif; t=028857504aa85b7281b1625a0bee47098; societyguester=028857504aa85b7281b1625a0bee47098; id=974361808; xnsid=c74ef2cb; ver=7.0; loginfrom=null; wp_fold=0"
cookies = {i.split("=")[0]: i.split("=")[1] for i in cookies.split("; ")}
```
```python
[self.url_temp.format(i*50) for i in range(1000)]


```
### 获取登录后的页面的三种方式
- 实例化session，使用session发送post请求，在使用他获取登录后的页面
- headers中添加cookie键，值为字符串
- 在请求方法中添加cookies参数，接受字典形式的cookie．字典形式的cookie中的键是cookie的name, 值是cookie的 value

[python-requests文档](https://requests.readthedocs.io/en/master/)
[python-requests中文文档](https://2.python-requests.org/zh_CN/latest/)
 
 
### 寻找登录的post地址
- 在from表单中寻找action对应的url地址
- post的数据是input标签中的name得知作为键，真正的用户名密码作为键的字典，post的url地址就是action中对应的url地址

-抓包，寻找登录的url地址
    - 勾选perserve　log地址，防止页面跳转找不到url 
    - 寻找post数据，确定参数
        -　参数不会变，直接用，比如密码不是动态加密的 
        -　参数会变
            - 参数在响应中 
            -  通过js生成
   
### 定位想要的js   
- 选择会触发js时间的按钮，点击event listener,找到js的位置 
- 通过chrome中的search all file来搜索url中的关键字
- 通过断电的方式查看js的操作

 
## requests小技巧
- 1. requests.util.dict_from_cookiejar把cookie对象转化为字典 
-  2. 请求SSL证书验证
    response = requests.get("https://www.12306.cn/mormhweb", verify=Flase)
-  设置超时
    response = requests.get(url, timeout=10)
-  4. 配合状态码判断是否 请求成功
    assert response.status_code == 200
-  
 
 
 
 
 
 
